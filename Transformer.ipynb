{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dealing with Sequences: Recurrent Neural Network (RNN) layer\n",
    "\n",
    "For a function that takes \n",
    "sequence $\\x^\\ip$ as input\n",
    "and creates sequence $\\y$ as  output we had two choices for implementing the function.\n",
    "\n",
    "The RNN implements the function as a \"loop\"\n",
    "- A function that taking **a single** $\\x_\\tp$ as input a time\n",
    "- Outputting $\\y_\\tp$ \n",
    "- Using a \"latent state\" $\\h_\\tp$  to summarize the prefix $\\x_{(1\\ldots \\tt)}$\n",
    "- Repeat in a loop over $\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\pr{\\h_\\tp | \\x_\\tp, \\h_{(\\tt-1)} } & \\text{latent variable } \\h_\\tp \\text{encodes } [ \\x_{(1)} \\dots \\x_\\tp ]\\\\\n",
    "\\pr{\\y_\\tp | \\h_\\tp }              & \\text{prediction contingent on latent variable} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "    \n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Loop with latent state</strong></center>\n",
    "    <img src=\"images/RNN_arch_loop.png\" width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Unrolling\" the loop makes it equivalent to a multi-layer network\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN unrolled</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_many_to_many.jpg\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer layer\n",
    "\n",
    "The alternative to the loop was to create a \"direct function\"\n",
    "- Taking a **sequence** $\\x_{(1 \\dots \\tt)}$ as input\n",
    "- Outputting $\\y_\\tp$\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Direct function</strong></center>\n",
    "    <img src=\"images/RNN_arch_parallel.png\" width=50%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to output the sequence $\\y_{(1)} \\ldots \\y_{(T)}$ we\n",
    "create $T$ copies of the function (one for each $\\y_\\tp$)\n",
    "- computes each $\\y_\\tp$ in **parallel**, not sequentially as in the loop\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Direct function, in parallel (masked input)</strong></center>\n",
    "<img src=\"images/Transformer_parallel_masked.png\" width=50%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The parallel units constitute a *Transformer layer*\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer layer (masked)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_1.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compared to the unrolled RNN, the Transformer layer\n",
    "- Has **no** data (e.g., $\\h_\\tp)$ passing from the computation between time steps (e.g., from $\\tt$ to $(\\tt +1)$)\n",
    "- Takes a **sequence** $\\x_{(1..t)}$ as input\n",
    "    - Because $\\y_\\tp$ is computed as a *direct* function of the prefix $\\x_{(1..t)}$ rather than recursively\n",
    "- Outputs generated in parallel, not sequentially\n",
    "- No gradients flowing backward over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With this architecture, we can compute more general functions\n",
    "- where each $\\y_\\tp$ depends on the entire $\\x_{(1 \\ldots T)}$ rather than a prefix $\\x_{(1 \\ldots \\tt)}$\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Direct function, in parallel (un-masked input)</strong></center>\n",
    "<img src=\"images/Transformer_parallel.png\" width=50%>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer layer</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_2.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate the different types of functions:\n",
    "- In a \"predict the next\" element function, we restrict the input to a causal prefix\n",
    "- In a \"summarize\" the sequence function, we allow access to the full sequence\n",
    "    - Context Sensitive Encoding of a word within a sentence\n",
    "    - Same effect as a bi-directional RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For generality\n",
    "- The Transformer input is usually the entire sequence $\\x_{(1 \\ldots T)}$ \n",
    "- When we need to \"hide\" part of the sequence, we can use an **input mask**\n",
    "- The mask can be arbitrary\n",
    "- The particular input mask restricting to a prefix implements **causal** masking\n",
    "    - Can't \"look into the future inputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Self Attention\n",
    "\n",
    "If we look inside the box computing the direct function, we will find several layers\n",
    "- An Attention layer\n",
    "    - To influence which elements of the input sequence $\\x$ to attend/focus when outputting $\\y_\\tp$\n",
    "- A Feed Forward Network (FF) layer to compute the function\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Encoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder.png\" width=60%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An Attention layer that attends/focus on its inputs implements what is called *Self-attention*\n",
    "- We will soon see the possibility of attending to other values\n",
    "\n",
    "If the function for $\\y_\\tp$ is restricted to prefix $\\x_{(1 \\ldots \\tt)}$ the Attention layer can use causal masking.\n",
    "\n",
    "This is referred to as *Masked Self-Attention*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross Attention\n",
    "\n",
    "It is common to use two Transformers in an Encoder-Decoder configuration.\n",
    "\n",
    "Recall the Encoder-Decoder architecture (using RNN's rather than Transformers in the diagram)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center><strong>Encoder-Decoder for language translation</strong></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder_Language_Translation.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Decoder in the Encoder-Decoder architecture is *generative*\n",
    "- Outputs $\\hat \\y_\\tp$ for a single $\\tt$ at a time\n",
    "- Appending output $\\hat \\y_\\tp$ to the input available to output the next $\\hat \\y_{(\\tt+1)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Encoder in the Encoder-Decoder architecture creates a latent state $\\bar \\h_\\tp$ which summarizes the input prefix $\\x_{(1 \\ldots \\tt)}$.\n",
    "\n",
    "In the above diagram the Decoder only has access to $\\bar\\h_{(\\bar T)}$,\n",
    "the final latent state\n",
    "- summarizing the entire input sequence\n",
    "\n",
    "This is very restrictive, forcing $\\bar \\h_{(\\bar T)}$ to encode a lot of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But we motivated Attention by suggesting that the Decoder have access to *each* $\\bar \\h_\\tp$ for $1 \\le \\tt \\le \\bar T$.\n",
    "- and use the Attention mechanism to decide which $\\bar \\h_\\tp$ to focus on when generating $\\hat \\y_\\tp$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder: Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Decoder_attention.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus the Decoder Transformer can also attend to the output of the Encoder.\n",
    "\n",
    "This is called *Cross Attention* (Encoder-Decoder Attention).\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Decoder.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder_Decoder_2.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Explanation of diagram**\n",
    "- The Encoder uses Self-attention (<span style=\"color:green\">wide Green arrow</span>) to attend to input sequence $\\x$\n",
    "- The Decoder uses Masked Self-attention (<span style=\"color:red\">wide Red arrow</span>) to attend to its input\n",
    "    - It's input is the prefix of the output sequence $\\y$\n",
    "    - Limited to prefix of length $\\tt$ by **masking**\n",
    "- The Decoder uses Cross Attention (between Encoder and Decoder) <span style=\"color:blue\">(wide Blue arrow)</span>\n",
    "    - To enable Decoder to focus on which Encoder latent state $\\bar \\h_\\tp$ to atttend to\n",
    "- The dotted <span style=\"color:blue\">(thin Blue arrow)</span> indicates that the output $\\hat \\y_\\tp$ is appended to the input that is available when generating $\\hat \\y_{(\\tt+1)}$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stacked Transformer\n",
    "\n",
    "Just as with many other layer types (e.g., RNN), we may stack Transformer layers.\n",
    "- Each layer creating alternate representations of the input of increasing complexity\n",
    "\n",
    "In fact, stacking $N > 1$  Transformer layers is typical.\n",
    "\n",
    "$N = 6$ was the choice of the original paper.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Stacked Transformer Layers (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder_Decoder_multi.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Full Encoder-Decoder Transformer architecture\n",
    "\n",
    "There are other components of the Encoder and Decoder that we have yet to describe.\n",
    "\n",
    "We will do so briefly.\n",
    "\n",
    "(The Transformer was introduced in the paper [Attention is all you Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "   \n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_is_all_u_need_Transformer.png\" width=60%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Embedding layers**\n",
    "\n",
    "We will motivate and describe Embeddings in the NLP module.\n",
    "\n",
    "For now:\n",
    "- an embedding is an encoding of a categorical value that is shorter than OHE\n",
    "\n",
    "It is used in the Transformer to\n",
    "- encode the input sequence of words\n",
    "- encode the output sequence of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Positional Encoding**\n",
    "\n",
    "The inputs are ordered (i.e., sequences) and thus describe a relative ordering relationship between elements.\n",
    "\n",
    "But inputs to most layer types (e.g., Fully Connected) are unordered.\n",
    "\n",
    "The Positional Encoding is a way of encoding the the relative ordering of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Add and Norm**\n",
    "\n",
    "We have seen each of these layer types before\n",
    "- Norm: Batch (or other) Normalization layers\n",
    "- Add: the part of the residual network that joins outputs of multiple previous layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The diagram shows an Encoder-Decoder pair.\n",
    "\n",
    "You will notice that each element of the pair is different.\n",
    "\n",
    "- It is possible to use each element independently as well.\n",
    "\n",
    "- But first we need to understand\n",
    "the source of the differences and their implications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder-style Transformer\n",
    "\n",
    "The Transformer for the Encoder and Decoder of an Encoder-Decoder Transformer are slightly different.\n",
    "\n",
    "They can also be used individually as well as in pairs.\n",
    "\n",
    "It's important to understand the differences in order to know when to use each individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Encoder side of the pair **does not** restrict the order in which it's inputs are accessed.\n",
    "- Self-attention **without** causal masking\n",
    "\n",
    "So the Encoder is appropriate for tasks that require a context-sensitive representation of\n",
    "each input element.\n",
    "\n",
    "For example: the meaning of the word \"**it**\" changes with a small change to a subsequent word in the following sentences:\n",
    "- \"The animal didn't cross the road because **it** was too tired\"\n",
    "\n",
    "- \"The animal didn't cross the road because **it** was too wide\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some tasks with this characteristic are\n",
    "- Sentiment\n",
    "- Masked Language Modeling: fill-in the masked word\n",
    "- Semantic Search\n",
    "    - compare a summary of the sequence that is the context-sensitive representation of\n",
    "        - query sentence\n",
    "        - document sentences\n",
    "    - Each summary is a kind of **sentence embedding**\n",
    "    - Summary\n",
    "        - pooling over each word\n",
    "        - final token\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decoder-style Transformer\n",
    "\n",
    "One notable aspect of the Decoder is its recurrent (generative) architecture\n",
    "- Output $\\y_{(\\tt-1)}$ is appended to the Decoder inputs available at step $\\tt$.\n",
    "    - The Decoder inputs are $\\y_{(1..T)}$, where $T$ is the full length of the Decoder output\n",
    "    - **But** Causal Masking ensures that only $\\y_{(1..\\tt)}$ is *available* at step $\\tt$.\n",
    "    \n",
    "Thus, the Decoder is appropriate for *generative* tasks\n",
    "- Text generation\n",
    "- Predict the next word in a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Advantages of a Transformer compared to an RNN\n",
    "\n",
    "Among the most important advantages of the Transformer over an RNN\n",
    "- are its ability to capture long-term dependencies \n",
    "- because all elements of the sequence are processed in parallel\n",
    "    - no vanishing gradient or truncated back propagation\n",
    "    \n",
    "This has made the Transformer the architecture of choice for NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The computational advantages (detailed in next section) are many:\n",
    "- Time: All steps computed in parallel\n",
    "    - $O(1)$ sequential steps versus $O(T)$\n",
    "- Fewer operations: faster training\n",
    "    - $O( T^2 * d )$ versus $O(T * d^2)$, where $d$ is size of latent state and length of a single input element\n",
    "        - e.g., $\\x_\\tp$ replaced by an embedding of dimension $d$\n",
    "    - Transformer has fewer operations when $T \\lt d$\n",
    "- Similar number of parameters \n",
    "    - When $T < \\sqrt{d}$: Self attention has about the same number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that, because of TBTT, T is the length of a *chunk* rather than the full input length\n",
    "- Typical $T = 64, d \\ge 256$\n",
    "\n",
    "So under the special case (that applies to sequences) that chunk length is short relative to representation size,\n",
    "it is not \"crazy\" to perform all elements of $\\x$ with separate FC's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The faster training enables\n",
    "- larger datasets\n",
    "- deeper models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detailed computational comparison of architectures\n",
    "\n",
    "| Type | Parameters  | Operations &nbsp; &nbsp; | Path length |\n",
    "|------|------       |------      |------       |\n",
    "|  CNN | $k * d^2$   | $T * k * d^2$ | $T$ |\n",
    "| RNN  | $d^2$       | $T * d^2$     | $T$ |\n",
    "| Self-attention | $T^2 *d $ | $T^2 *d$ | 1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's the details of the math        \n",
    "\n",
    "Attention involves a dot product (of vectors of length $d$)\n",
    "- Each input matched against all others: $T * T$\n",
    "- So $T^2 *d$ operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNN\n",
    "- $T$ sequential steps: path length $T$ \n",
    "- Each step evaluates\n",
    "    $$\n",
    "\\h_\\tp  =  \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(t-1)}  + \\b_h) \n",
    "$$\n",
    "- $\\h_\\tp$ has multiple elements, assume $|| \\h || = O(d)$\n",
    "    - Computing updated hidden state element $j$ (i.e., $\\h_{\\tp, j}$) involves dot product of vectors of length $d$ (size of $\\x_\\tp)$\n",
    "    - $d$ multiplications per element of $\\h$, times $O(d)$ elements of $\\h$ is $O(d^2)$ per step\n",
    "    - So $T * d^2$ operations\n",
    "    \n",
    "- $\\W_{hh}$ matrix: $d^2$ parameters\n",
    "  - $ | \\h | = d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "CNN\n",
    "- path length $T$ \n",
    "    - each kernel multiplication connects only $k$ elements of $\\x$\n",
    "    - since kernels overlap inputs, can't parallelize, hence $O(T/k)$ path length\n",
    "        - can reduce to $\\log(T)$ with tree structure\n",
    "- Parameters\n",
    "    - kernel size $k$\n",
    "    - number of input channels = number of output channels = $d$\n",
    "    - $k *d$ parameters for kernel of one channel\n",
    "    - $k * d^2$ parameters for kernel for all $d$ output channels\n",
    "    \n",
    "- Operations\n",
    "    - for a single output channel: $k$ per input channel\n",
    "        - There are $d$ input channels, so $k *d$ for each dot product of *one* output channel\n",
    "        - There are $d$ output channels, so $k * d^2$ per time step\n",
    "    - $T$ time steps so $T * k * d^2$ number of operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A free lunch ? Almost !\n",
    "\n",
    "Transformers offer the possibility of great improvements in training speed\n",
    "- Parallelism\n",
    "- Fewer operations\n",
    "    \n",
    "Sounds too good to be true.  Is there such a thing as a free lunch ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Almost\n",
    "- RNN can handle sequences of arbitrary length ($T$ unbounded)\n",
    "- Transformer has a fixed number of parallel units, which limits the length of sequences\n",
    "\n",
    "But, in practice: RNN uses *Truncated* Back Propagation Through Time\n",
    "- So the maximum distance between input sequence elements is bounded by $k$, the truncation length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some other advantages\n",
    "\n",
    "- Can learn long-range dependencies\n",
    "    - Gradients within a layer don't flow backwards: always a single step\n",
    "        - Can't vanish or explode\n",
    "    - The output $\\y^{[\\ll]}_\\tp$ of layer $\\ll$ (for stacked Transformer layers) is a function of **all** inputs\n",
    "    $$\n",
    "    \\y^{[\\ll-1]}_{(\\tt')} \\text{ for } 1 \\le \\tt' \\le T\n",
    "    $$\n",
    "        - so can directly access a distant input\n",
    "        - not diminished by passing through multiple intermediate time steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some drawbacks\n",
    "\n",
    "- The output $\\y^{[\\ll]}_\\tp$ of layer $\\ll$ (for stacked Transformer layers) is a function of **all** inputs, **always**\n",
    "    - Perhaps less efficient\n",
    "- Unless you add positional encoding, you lose ordering relationships between inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
